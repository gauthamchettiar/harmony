[{"source": "readme", "content": [{"Setting": "credentials_path", "Required": false, "Default": "None", "Description": "The path to a gcp credentials json file."}, {"Setting": "credentials_json", "Required": false, "Default": "None", "Description": "A JSON string of your service account JSON file."}, {"Setting": "project", "Required": true, "Default": "None", "Description": "The target GCP project to materialize data into."}, {"Setting": "dataset", "Required": true, "Default": "None", "Description": "The target dataset to materialize data into."}, {"Setting": "location", "Required": false, "Default": "US", "Description": "The target dataset location to materialize data into. Applies also to the GCS bucket if using gcs_stage load method."}, {"Setting": "batch_size", "Required": false, "Default": "500", "Description": "The maximum number of rows to send in a single batch to the worker. This should be configured based on load method. For storage_write_api and streaming_insert it should be <=500, for the LoadJob sinks, it can be much higher, ie >100,000"}, {"Setting": "timeout", "Required": false, "Default": "600", "Description": "Default timeout for batch_job and gcs_stage derived LoadJobs."}, {"Setting": "fail_fast", "Required": false, "Default": "True", "Description": "Fail the entire load job if any row fails to insert."}, {"Setting": "denormalized", "Required": false, "Default": "False", "Description": "Determines whether to denormalize the data before writing to BigQuery. A false value will write data using a fixed JSON column based schema, while a true value will write data using a dynamic schema derived from the tap."}, {"Setting": "method", "Required": true, "Default": "storage_write_api", "Description": "The method to use for writing to BigQuery. Must be one of batch_job, storage_write_api, gcs_stage, streaming_insert"}, {"Setting": "generate_view", "Required": false, "Default": "False", "Description": "Determines whether to generate a view based on the SCHEMA message parsed from the tap. Only valid if denormalized=false meaning you are using the fixed JSON column based schema."}, {"Setting": "upsert", "Required": false, "Default": "False", "Description": "Determines if we should upsert. Defaults to false. A value of true will write to a temporary table and then merge into the target table (upsert). This requires the target table to be unique on the key properties. A value of false will write to the target table directly (append). A value of an array of strings will evaluate the strings in order using fnmatch. At the end of the array, the value of the last match will be used. If not matched, the default value is false (append)."}, {"Setting": "overwrite", "Required": false, "Default": "False", "Description": "Determines if the target table should be overwritten on load. Defaults to false. A value of true will write to a temporary table and then overwrite the target table inside a transaction (so it is safe). A value of false will write to the target table directly (append). A value of an array of strings will evaluate the strings in order using fnmatch. At the end of the array, the value of the last match will be used. If not matched, the default value is false. This is mutually exclusive with the upsert option. If both are set, upsert will take precedence."}, {"Setting": "dedupe_before_upsert", "Required": false, "Default": "False", "Description": "This option is only used if upsert is enabled for a stream. The selection criteria for the stream's candidacy is the same as upsert. If the stream is marked for deduping before upsert, we will create a _session scoped temporary table during the merge transaction to dedupe the ingested records. This is useful for streams that are not unique on the key properties during an ingest but are unique in the source system. Data lake ingestion is often a good example of this where the same unique record may exist in the lake at different points in time from different extracts."}, {"Setting": "bucket", "Required": false, "Default": "None", "Description": "The GCS bucket to use for staging data. Only used if method is gcs_stage."}, {"Setting": "cluster_on_key_properties", "Required": false, "Default": "0", "Description": "Determines whether to cluster on the key properties from the tap. Defaults to false. When false, clustering will be based on _sdc_batched_at instead."}, {"Setting": "partition_granularity", "Required": false, "Default": "\"month\"", "Description": "Indicates the granularity of the created table partitioning scheme which is based on _sdc_batched_at. By default the granularity is monthly. Must be one of: \"hour\", \"day\", \"month\", \"year\"."}, {"Setting": "column_name_transforms.lower", "Required": false, "Default": "None", "Description": "Lowercase column names."}, {"Setting": "column_name_transforms.quote", "Required": false, "Default": "None", "Description": "Quote column names in any generated DDL."}, {"Setting": "column_name_transforms.add_underscore_when_invalid", "Required": false, "Default": "None", "Description": "Add an underscore to the column name if it starts with a digit to make it valid."}, {"Setting": "column_name_transforms.snake_case", "Required": false, "Default": "None", "Description": "Snake case all incoming column names. Does not apply to fixed schema loads but does apply to the view auto-generated over them."}, {"Setting": "options.storage_write_batch_mode", "Required": false, "Default": "None", "Description": "By default, we use the default stream (Committed mode) in the storage_write_api load method which results in streaming records which are immediately available and is generally fastest. If this is set to true, we will use the application created streams (pending mode) to transactionally batch data on STATE messages and at end of pipe."}, {"Setting": "options.process_pool", "Required": false, "Default": "None", "Description": "By default we use an autoscaling threadpool to write to BigQuery. If set to true, we will use a process pool."}, {"Setting": "options.max_workers", "Required": false, "Default": "None", "Description": "By default, each sink type has a preconfigured max worker pool limit. This sets an override for maximum number of workers in the pool."}, {"Setting": "schema_resolver_version", "Required": false, "Default": "1", "Description": "The version of the schema resolver to use. Defaults to 1. Version 2 uses JSON as a fallback during denormalization. This only has an effect if denormalized=true"}, {"Setting": "stream_maps", "Required": false, "Default": "None", "Description": "Config object for stream maps capability. For more information check out Stream Maps."}, {"Setting": "stream_map_config", "Required": false, "Default": "None", "Description": "User-defined config values to be used within map expressions."}, {"Setting": "flattening_enabled", "Required": false, "Default": "None", "Description": "'True' to enable schema flattening and automatically expand nested properties."}, {"Setting": "flattening_max_depth", "Required": false, "Default": "None", "Description": "The max depth to flatten schemas."}], "instance": 0}]